{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4ba04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH='../../dataset/train_feat.csv'\n",
    "TEST_CSV_PATH='../../dataset/test_feat.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5039c9ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T19:45:05.324560Z",
     "iopub.status.busy": "2025-09-19T19:45:05.324119Z",
     "iopub.status.idle": "2025-09-19T19:45:05.353188Z",
     "shell.execute_reply": "2025-09-19T19:45:05.352427Z"
    },
    "papermill": {
     "duration": 0.035464,
     "end_time": "2025-09-19T19:45:05.354803",
     "exception": false,
     "start_time": "2025-09-19T19:45:05.319339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def featurizer(train_df,test_df):\n",
    "    train_df[\"W/L\"]=train_df[\"Width (mm)\"]/train_df[\"Length (mm)\"]\n",
    "    train_df[\"L/T\"]=train_df[\"Length (mm)\"]/train_df[\"Thickness (mm)\"]\n",
    "    train_df[\"W/T\"]=train_df[\"Thickness (mm)\"]/train_df[\"Width (mm)\"]\n",
    "    train_df[\"Total_Alloy_wt\"] = train_df[[\n",
    "        \"C, wt.%\", \"Si, wt.%\", \"Mn, wt.%\", \"P, wt.%\", \"S, wt.%\",\n",
    "        \"Ni, wt.%\", \"Cr, wt.%\", \"Mo, wt.%\", \"Al, wt.%\", \"N, wt.%\",\n",
    "        \"Ti, wt.%\", \"Nb, wt.%\", \"B, wt.%\", \"Cu, wt.%\", \"V, wt.%\",\n",
    "        \"Zr, wt.%\", \"W, wt.%\", \"Sn, wt.%\", \"Co, wt.%\", \"O, wt,%\",\n",
    "        \"Ta, wt.%\", \"As, wt%\", \"Sb, wt.%\"\n",
    "    ]].sum(axis=1)\n",
    "\n",
    "    train_df[\"Embrittling_Elements\"] = (\n",
    "        train_df[\"P, wt.%\"] +\n",
    "        train_df[\"S, wt.%\"] +\n",
    "        train_df[\"Cu, wt.%\"] +\n",
    "        train_df[\"As, wt%\"] +\n",
    "        train_df[\"Sb, wt.%\"] +\n",
    "        train_df[\"B, wt.%\"]\n",
    "    )\n",
    "\n",
    "    train_df[\"Toughness_Enhancers\"] = (\n",
    "        train_df[\"Ni, wt.%\"] +\n",
    "        train_df[\"Mn, wt.%\"] +\n",
    "        train_df[\"Mo, wt.%\"] +\n",
    "        train_df[\"V, wt.%\"] +\n",
    "        train_df[\"Ti, wt.%\"] +\n",
    "        train_df[\"Nb, wt.%\"]\n",
    "    )\n",
    "\n",
    "    train_df[\"Hardener_Elements\"] = (\n",
    "        train_df[\"C, wt.%\"] +\n",
    "        train_df[\"Cr, wt.%\"] +\n",
    "        train_df[\"Mo, wt.%\"] +\n",
    "        train_df[\"V, wt.%\"] +\n",
    "        train_df[\"W, wt.%\"]\n",
    "    )\n",
    "\n",
    "    train_df[\"Aging_Sensitive\"] = (\n",
    "        train_df[\"Cu, wt.%\"] +\n",
    "        train_df[\"Ni, wt.%\"] +\n",
    "        train_df[\"P, wt.%\"]\n",
    "    )\n",
    "\n",
    "    train_df[\"Carbon_Equivalent\"] = (\n",
    "        train_df[\"C, wt.%\"] +\n",
    "        (train_df[\"Mn, wt.%\"] + train_df[\"Si, wt.%\"]) / 6 +\n",
    "        (train_df[\"Cr, wt.%\"] + train_df[\"Mo, wt.%\"] + train_df[\"V, wt.%\"]) / 5 +\n",
    "        (train_df[\"Ni, wt.%\"] + train_df[\"Cu, wt.%\"]) / 15\n",
    "    )\n",
    "\n",
    "    train_df[\"Fe_Deficit\"] = 100 - train_df[\"Fe, wt.%\"]\n",
    "\n",
    "    # Compute derived features for alloy analysis\n",
    "    test_df[\"W/L\"]=test_df[\"Width (mm)\"]/test_df[\"Length (mm)\"]\n",
    "    test_df[\"L/T\"]=test_df[\"Length (mm)\"]/test_df[\"Thickness (mm)\"]\n",
    "    test_df[\"W/T\"]=test_df[\"Thickness (mm)\"]/test_df[\"Width (mm)\"]\n",
    "    test_df[\"Total_Alloy_wt\"] = test_df[[\n",
    "        \"C, wt.%\", \"Si, wt.%\", \"Mn, wt.%\", \"P, wt.%\", \"S, wt.%\",\n",
    "        \"Ni, wt.%\", \"Cr, wt.%\", \"Mo, wt.%\", \"Al, wt.%\", \"N, wt.%\",\n",
    "        \"Ti, wt.%\", \"Nb, wt.%\", \"B, wt.%\", \"Cu, wt.%\", \"V, wt.%\",\n",
    "        \"Zr, wt.%\", \"W, wt.%\", \"Sn, wt.%\", \"Co, wt.%\", \"O, wt,%\",\n",
    "        \"Ta, wt.%\", \"As, wt%\", \"Sb, wt.%\"\n",
    "    ]].sum(axis=1)\n",
    "    test_df[\"Embrittling_Elements\"] = (\n",
    "        test_df[\"P, wt.%\"] +\n",
    "        test_df[\"S, wt.%\"] +\n",
    "        test_df[\"Cu, wt.%\"] +\n",
    "        test_df[\"As, wt%\"] +\n",
    "        test_df[\"Sb, wt.%\"] +\n",
    "        test_df[\"B, wt.%\"]\n",
    "    )\n",
    "\n",
    "    test_df[\"Toughness_Enhancers\"] = (\n",
    "        test_df[\"Ni, wt.%\"] +\n",
    "        test_df[\"Mn, wt.%\"] +\n",
    "        test_df[\"Mo, wt.%\"] +\n",
    "        test_df[\"V, wt.%\"] +\n",
    "        test_df[\"Ti, wt.%\"] +\n",
    "        test_df[\"Nb, wt.%\"]\n",
    "    )\n",
    "\n",
    "    test_df[\"Hardener_Elements\"] = (\n",
    "        test_df[\"C, wt.%\"] +\n",
    "        test_df[\"Cr, wt.%\"] +\n",
    "        test_df[\"Mo, wt.%\"] +\n",
    "        test_df[\"V, wt.%\"] +\n",
    "        test_df[\"W, wt.%\"]\n",
    "    )\n",
    "\n",
    "    test_df[\"Aging_Sensitive\"] = (\n",
    "        test_df[\"Cu, wt.%\"] +\n",
    "        test_df[\"Ni, wt.%\"] +\n",
    "        test_df[\"P, wt.%\"]\n",
    "    )\n",
    "\n",
    "    test_df[\"Carbon_Equivalent\"] = (\n",
    "        test_df[\"C, wt.%\"] +\n",
    "        (test_df[\"Mn, wt.%\"] + test_df[\"Si, wt.%\"]) / 6 +\n",
    "        (test_df[\"Cr, wt.%\"] + test_df[\"Mo, wt.%\"] + test_df[\"V, wt.%\"]) / 5 +\n",
    "        (test_df[\"Ni, wt.%\"] + test_df[\"Cu, wt.%\"]) / 15\n",
    "    )\n",
    "\n",
    "    test_df[\"Fe_Deficit\"] = 100 - test_df[\"Fe, wt.%\"]\n",
    "\n",
    "    eps = 1e-6  # small value to avoid divide-by-zero\n",
    "\n",
    "\n",
    "    # 1. Composition Ratios\n",
    "    train_df['C_Mn'] = train_df['C, wt.%'] / (train_df['Mn, wt.%'] + eps)\n",
    "    train_df['Cr_Mo_V_Ni'] = train_df[['Cr, wt.%', 'Mo, wt.%', 'V, wt.%', 'Ni, wt.%']].sum(axis=1)\n",
    "    train_df['NiMn_to_CrMoV'] = (train_df['Ni, wt.%'] + train_df['Mn, wt.%']) / (train_df['Cr, wt.%'] + train_df['Mo, wt.%'] + train_df['V, wt.%'] + eps)\n",
    "    train_df['Ni_to_Fe'] = train_df['Ni, wt.%'] / (train_df['Fe, wt.%'] + eps)\n",
    "    train_df['Cr_to_Fe'] = train_df['Cr, wt.%'] / (train_df['Fe, wt.%'] + eps)\n",
    "    train_df['Mo_to_Fe'] = train_df['Mo, wt.%'] / (train_df['Fe, wt.%'] + eps)\n",
    "\n",
    "    # 2. Geometry & Stress Features\n",
    "    train_df['Volume'] = train_df['Length (mm)'] * train_df['Width (mm)'] * train_df['Thickness (mm)']\n",
    "    train_df['Surface_Area'] = 2 * (\n",
    "        train_df['Length (mm)'] * train_df['Width (mm)'] +\n",
    "        train_df['Length (mm)'] * train_df['Thickness (mm)'] +\n",
    "        train_df['Width (mm)'] * train_df['Thickness (mm)']\n",
    "    )\n",
    "    train_df['Compactness'] = train_df['Volume'] / (train_df['Surface_Area'] + eps)\n",
    "\n",
    "    # Compute missing 'Area' as cross-sectional area\n",
    "    train_df['Area'] = train_df['Width (mm)'] * train_df['Thickness (mm)']\n",
    "    train_df['Impact_energy_per_Area'] = train_df['Impact energy (J)'] / (train_df['Area'] + eps)\n",
    "    train_df['Impact_energy_per_Volume'] = train_df['Impact energy (J)'] / (train_df['Volume'] + eps)\n",
    "\n",
    "    # 3. Elemental diversity\n",
    "    element_cols = [col for col in train_df.columns if ', wt.%' in col]\n",
    "    norm_composition = train_df[element_cols].div(train_df[element_cols].sum(axis=1) + eps, axis=0)\n",
    "    train_df['Elemental_count'] = (train_df[element_cols] > 0).sum(axis=1)\n",
    "    train_df['Std_dev_wt'] = train_df[element_cols].std(axis=1)\n",
    "    train_df['Shannon_entropy_composition'] = -(norm_composition * np.log(norm_composition + eps)).sum(axis=1)\n",
    "\n",
    "    # 4. Interaction terms\n",
    "    train_df['C_times_Mn'] = train_df['C, wt.%'] * train_df['Mn, wt.%']\n",
    "    train_df['Mn_times_Cr'] = train_df['Mn, wt.%'] * train_df['Cr, wt.%']\n",
    "    train_df['Ni_times_Mo'] = train_df['Ni, wt.%'] * train_df['Mo, wt.%']\n",
    "\n",
    "    # 5. Physics-inspired placeholders\n",
    "    magpie_cols = train_df.columns[train_df.columns.str.contains('CovalentRadius') & train_df.columns.str.contains('range')]\n",
    "    if len(magpie_cols):\n",
    "        train_df['Solid_solution_strengthening'] = train_df[magpie_cols[0]] * train_df[element_cols].std(axis=1)\n",
    "    else:\n",
    "        train_df['Solid_solution_strengthening'] = np.nan\n",
    "\n",
    "    train_df['Hardness_index'] = (\n",
    "        0.5 * train_df['C, wt.%'] +\n",
    "        0.3 * train_df['Mn, wt.%'] +\n",
    "        0.2 * train_df['Mo, wt.%'] +\n",
    "        0.1 * train_df['Ni, wt.%']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 1. Composition Ratios\n",
    "    test_df['C_Mn'] = test_df['C, wt.%'] / (test_df['Mn, wt.%'] + eps)\n",
    "    test_df['Cr_Mo_V_Ni'] = test_df[['Cr, wt.%', 'Mo, wt.%', 'V, wt.%', 'Ni, wt.%']].sum(axis=1)\n",
    "    test_df['NiMn_to_CrMoV'] = (test_df['Ni, wt.%'] + test_df['Mn, wt.%']) / (test_df['Cr, wt.%'] + test_df['Mo, wt.%'] + test_df['V, wt.%'] + eps)\n",
    "    test_df['Ni_to_Fe'] = test_df['Ni, wt.%'] / (test_df['Fe, wt.%'] + eps)\n",
    "    test_df['Cr_to_Fe'] = test_df['Cr, wt.%'] / (test_df['Fe, wt.%'] + eps)\n",
    "    test_df['Mo_to_Fe'] = test_df['Mo, wt.%'] / (test_df['Fe, wt.%'] + eps)\n",
    "\n",
    "    # 2. Geometry & Stress Features\n",
    "    test_df['Volume'] = test_df['Length (mm)'] * test_df['Width (mm)'] * test_df['Thickness (mm)']\n",
    "    test_df['Surface_Area'] = 2 * (\n",
    "        test_df['Length (mm)'] * test_df['Width (mm)'] +\n",
    "        test_df['Length (mm)'] * test_df['Thickness (mm)'] +\n",
    "        test_df['Width (mm)'] * test_df['Thickness (mm)']\n",
    "    )\n",
    "    test_df['Compactness'] = test_df['Volume'] / (test_df['Surface_Area'] + eps)\n",
    "\n",
    "    # Compute missing 'Area' as cross-sectional area\n",
    "    test_df['Area'] = test_df['Width (mm)'] * test_df['Thickness (mm)']\n",
    "    test_df['Impact_energy_per_Area'] = test_df['Impact energy (J)'] / (test_df['Area'] + eps)\n",
    "    test_df['Impact_energy_per_Volume'] = test_df['Impact energy (J)'] / (test_df['Volume'] + eps)\n",
    "\n",
    "    # 3. Elemental diversity\n",
    "    element_cols = [col for col in test_df.columns if ', wt.%' in col]\n",
    "    norm_composition = test_df[element_cols].div(test_df[element_cols].sum(axis=1) + eps, axis=0)\n",
    "    test_df['Elemental_count'] = (test_df[element_cols] > 0).sum(axis=1)\n",
    "    test_df['Std_dev_wt'] = test_df[element_cols].std(axis=1)\n",
    "    test_df['Shannon_entropy_composition'] = -(norm_composition * np.log(norm_composition + eps)).sum(axis=1)\n",
    "\n",
    "    # 4. Interaction terms\n",
    "    test_df['C_times_Mn'] = test_df['C, wt.%'] * test_df['Mn, wt.%']\n",
    "    test_df['Mn_times_Cr'] = test_df['Mn, wt.%'] * test_df['Cr, wt.%']\n",
    "    test_df['Ni_times_Mo'] = test_df['Ni, wt.%'] * test_df['Mo, wt.%']\n",
    "\n",
    "    # 5. Physics-inspired placeholders\n",
    "    magpie_cols = test_df.columns[test_df.columns.str.contains('CovalentRadius') & test_df.columns.str.contains('range')]\n",
    "    if len(magpie_cols):\n",
    "        test_df['Solid_solution_strengthening'] = test_df[magpie_cols[0]] * test_df[element_cols].std(axis=1)\n",
    "    else:\n",
    "        test_df['Solid_solution_strengthening'] = np.nan\n",
    "\n",
    "    test_df['Hardness_index'] = (\n",
    "        0.5 * test_df['C, wt.%'] +\n",
    "        0.3 * test_df['Mn, wt.%'] +\n",
    "        0.2 * test_df['Mo, wt.%'] +\n",
    "        0.1 * test_df['Ni, wt.%']\n",
    "    )\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed32a5a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T19:45:05.361637Z",
     "iopub.status.busy": "2025-09-19T19:45:05.361310Z",
     "iopub.status.idle": "2025-09-19T19:45:05.367187Z",
     "shell.execute_reply": "2025-09-19T19:45:05.366444Z"
    },
    "papermill": {
     "duration": 0.010813,
     "end_time": "2025-09-19T19:45:05.368692",
     "exception": false,
     "start_time": "2025-09-19T19:45:05.357879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def median_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput=\"uniform_average\"):\n",
    "    \"\"\"\n",
    "    Median absolute percentage error regression loss.\n",
    "    Compatible with sklearn-style usage.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.array(y_pred, dtype=np.float64).ravel()\n",
    "\n",
    "    # avoid division by zero\n",
    "    nonzero_mask = y_true != 0\n",
    "    if not np.any(nonzero_mask):\n",
    "        raise ValueError(\"All y_true values are zero; MAPE cannot be calculated.\")\n",
    "\n",
    "    errors = np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])\n",
    "\n",
    "    return np.median(errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205ad335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T19:45:05.375050Z",
     "iopub.status.busy": "2025-09-19T19:45:05.374764Z",
     "iopub.status.idle": "2025-09-19T19:45:05.380804Z",
     "shell.execute_reply": "2025-09-19T19:45:05.379976Z"
    },
    "papermill": {
     "duration": 0.010863,
     "end_time": "2025-09-19T19:45:05.382317",
     "exception": false,
     "start_time": "2025-09-19T19:45:05.371454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_to_remove = []\n",
    "feature_to_include = []\n",
    "feature_to_include = [\"C, wt.%\",\t\"Si, wt.%\",\t\"Mn, wt.%\",\t\"P, wt.%\",\t\"S, wt.%\",\t\"Ni, wt.%\",\t\"Cr, wt.%\",\t\"Mo, wt.%\",\t\"Al, wt.%\",\t\"N, wt.%\",\t\"Ti, wt.%\",\t\"Fe, wt.%\",\t\"Nb, wt.%\",\t\"B, wt.%\",\t\"Cu, wt.%\",\t\"V, wt.%\",\t\"Zr, wt.%\",\t\"W, wt.%\",\t\"Sn, wt.%\",\t\"Co, wt.%\",\t\"O, wt,%\", \"Ta, wt.%\",\t\"As, wt%\",\"Sb, wt.%\",'MagpieData mean GSbandgap', 'MagpieData avg_dev GSbandgap', 'Test Temperature (C) ', 'MagpieData mean MeltingT', 'Solid_solution_strengthening', 'C_Mn', 'Embrittling_Elements', 'Impact_energy_per_Volume', 'MagpieData mean NdValence', 'Impact energy (J)', 'Impact_energy_per_Area', 'Compactness', 'MagpieData mean SpaceGroupNumber', 'Cr_to_Fe', 'MagpieData avg_dev NUnfilled', 'Area',  'MagpieData mean NValence', 'Surface_Area', 'Volume', 'MagpieData mean NUnfilled', 'MagpieData avg_dev NpUnfilled', 'MagpieData avg_dev NpValence', 'Aging_Sensitive', 'MagpieData avg_dev NsValence', 'MagpieData mean NpUnfilled', 'MagpieData avg_dev SpaceGroupNumber', 'MagpieData mean NsUnfilled', 'MagpieData mean NsValence', 'MagpieData avg_dev MeltingT', 'Ni_to_Fe', 'MagpieData mean NpValence', 'C_times_Mn', 'Elemental_count', 'NiMn_to_CrMoV', 'MagpieData avg_dev Column', 'MagpieData avg_dev NsUnfilled', 'MagpieData avg_dev NdUnfilled', 'MagpieData mean GSvolume_pa',  'MagpieData avg_dev MendeleevNumber', 'Cr_Mo_V_Ni', 'MagpieData mean Electronegativity', 'Mn_times_Cr', 'W/L', 'MagpieData mean CovalentRadius',  'L/T', 'Carbon_Equivalent', 'MagpieData avg_dev CovalentRadius']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00dcbba",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-19T19:45:05.388866Z",
     "iopub.status.busy": "2025-09-19T19:45:05.388572Z",
     "iopub.status.idle": "2025-09-19T20:07:40.112658Z",
     "shell.execute_reply": "2025-09-19T20:07:40.111836Z"
    },
    "papermill": {
     "duration": 1354.729748,
     "end_time": "2025-09-19T20:07:40.114613",
     "exception": false,
     "start_time": "2025-09-19T19:45:05.384865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/ksu-july-dataset/train_feat.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(out)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/ksu-july-dataset/train_feat.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/ksu-july-dataset/test_feat.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Feature engineering placeholders\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# train_df,test_df=featurizer(train_df,test_df)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# train_df, test_df = featurizer(train_df, test_df)  # define this if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\r\\anaconda3\\envs\\tansorflow\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\r\\anaconda3\\envs\\tansorflow\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\r\\anaconda3\\envs\\tansorflow\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\r\\anaconda3\\envs\\tansorflow\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\r\\anaconda3\\envs\\tansorflow\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ksu-july-dataset/train_feat.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "seed=911\n",
    "random.seed(seed)                         # Python RNG\n",
    "np.random.seed(seed)                      # NumPy RNG\n",
    "torch.manual_seed(seed)                   # PyTorch CPU RNG\n",
    "torch.cuda.manual_seed(seed)               # Current GPU RNG\n",
    "torch.cuda.manual_seed_all(seed)           # All GPU devices RNG\n",
    "\n",
    "# Make operations deterministic (slightly slower)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Define Model\n",
    "# -------------------------\n",
    "class MLPWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512]*6, embed_dim=256, num_heads=128, dropout=0.05):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.LeakyReLU(0.1))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.proj = nn.Linear(prev_dim, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.regressor = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        out = self.proj(out).unsqueeze(1)\n",
    "        attn_out, _ = self.attn(out, out, out)\n",
    "        out = attn_out.mean(dim=1)\n",
    "        return self.regressor(out).squeeze(-1)\n",
    "\n",
    "# -------------------------\n",
    "# Load Data\n",
    "# -------------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH).select_dtypes(include=\"number\")\n",
    "test_df = pd.read_csv('/kaggle/input/ksu-july-dataset/test_feat.csv').select_dtypes(include=\"number\")\n",
    "\n",
    "# -------------------------\n",
    "# Feature engineering placeholders\n",
    "# train_df,test_df=featurizer(train_df,test_df)\n",
    "\n",
    "# -------------------------\n",
    "# train_df, test_df = featurizer(train_df, test_df)  # define this if needed\n",
    "cols_to_drop = ['DBTT', 'USE', \"Width (mm)\", \"Length (mm)\", \"Thickness (mm)\"]\n",
    "\n",
    "target = \"DBTT\"\n",
    "\n",
    "# -------------------------\n",
    "# Filter outliers in target\n",
    "# -------------------------\n",
    "mean_val = train_df[target].mean()\n",
    "std_val = train_df[target].std()\n",
    "# lower_bound = mean_val - 2.25 * std_val\n",
    "upper_bound = mean_val + 1.75 * std_val\n",
    "\n",
    "train_df = train_df.dropna(subset=[target])\n",
    "if feature_to_include:\n",
    "    existing_features = [f for f in feature_to_include if f in train_df.columns]\n",
    "    train_df = train_df[existing_features + [target]]\n",
    "print(train_df.shape)\n",
    "# train_df = train_df[(train_df[target] >= lower_bound) & (train_df[target] <= upper_bound)]\n",
    "train_df = train_df[ (train_df[target] <= upper_bound)]\n",
    "print(train_df.shape)\n",
    "\n",
    "test_df = test_df.dropna(subset=[target])\n",
    "test_df = test_df[list(train_df.columns)]\n",
    "# mean_val = test_df[target].mean()\n",
    "# std_val = test_df[target].std()\n",
    "# upper_bound = mean_val + 1.5 * std_val\n",
    "test_df = test_df[ (test_df[target] <= upper_bound)]\n",
    "\n",
    "# -------------------------\n",
    "# Prepare data\n",
    "# -------------------------\n",
    "X = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "y = train_df[target]\n",
    "X_test = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "y_test = test_df[target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = PowerTransformer(method='yeo-johnson')\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1)).ravel()\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1,1)).ravel()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1)).ravel()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "model = MLPWithAttention(input_dim=X_train.shape[1]).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "max_epochs = 2000\n",
    "batch_size = 256\n",
    "\n",
    "train_mae_hist, val_mae_hist, test_mae_hist = [], [], []\n",
    "best_train_mae, best_val_mae, best_test_mae = float(\"inf\"),float(\"inf\"),float(\"inf\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train_tensor.size(0))\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor[idx])\n",
    "        loss = criterion(outputs, y_train_tensor[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = scaler_y.inverse_transform(model(X_train_tensor).cpu().numpy().reshape(-1,1)).ravel()\n",
    "        val_pred = scaler_y.inverse_transform(model(X_val_tensor).cpu().numpy().reshape(-1,1)).ravel()\n",
    "        test_pred = scaler_y.inverse_transform(model(X_test_tensor).cpu().numpy().reshape(-1,1)).ravel()\n",
    "\n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        val_mae = mean_absolute_error(y_val, val_pred)\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "\n",
    "    train_mae_hist.append(train_mae)\n",
    "    val_mae_hist.append(val_mae)\n",
    "    test_mae_hist.append(test_mae)\n",
    "\n",
    "    if best_train_mae>train_mae:\n",
    "        best_train_mae=train_mae\n",
    "        \n",
    "    if best_test_mae>test_mae:\n",
    "        torch.save(model.state_dict(), \"mlp_with_attention.pth\")\n",
    "        best_test_mae=test_mae\n",
    "        \n",
    "    if best_val_mae>val_mae:\n",
    "        best_val_mae=val_mae\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Train MAE: {best_train_mae:.4f}, Val MAE: {best_val_mae:.4f}, Test MAE: {best_test_mae:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plot\n",
    "# -------------------------\n",
    "plt.plot(train_mae_hist, label=\"Train MAE\")\n",
    "plt.plot(val_mae_hist, label=\"Validation MAE\")\n",
    "plt.plot(test_mae_hist, label=\"Test MAE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.title(\"Train vs Validation MAE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14956ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T20:07:40.124539Z",
     "iopub.status.busy": "2025-09-19T20:07:40.124058Z",
     "iopub.status.idle": "2025-09-19T20:07:40.132016Z",
     "shell.execute_reply": "2025-09-19T20:07:40.131098Z"
    },
    "papermill": {
     "duration": 0.014661,
     "end_time": "2025-09-19T20:07:40.133651",
     "exception": false,
     "start_time": "2025-09-19T20:07:40.118990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "def score_generator(X, y, model, res_type, is_torch=True, X_scaler=None, y_scaler=None):\n",
    "    if is_torch:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_scaled = model(X).cpu().numpy()\n",
    "\n",
    "        # ensure 2D for inverse_transform\n",
    "        if y_scaler is not None:\n",
    "            y_pred_scaled = y_pred_scaled.reshape(-1, 1)\n",
    "            y_pred = y_scaler.inverse_transform(y_pred_scaled).ravel()\n",
    "        else:\n",
    "            y_pred = y_pred_scaled.ravel()\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "    # metrics\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    mape = median_absolute_percentage_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    df=pd.DataFrame({\"true_value\":y,\"pred_value\":y_pred,})\n",
    "    df[\"error\"]=abs(df[\"true_value\"]-df[\"pred_value\"])\n",
    "    df.to_csv(f\"{res_type}_with_feature_results.csv\")\n",
    "    print(df[\"error\"].mean())\n",
    "    print(f\"{res_type} -> MAE: {mae:.4f}, MAPE: {mape:.4f}, R2: {r2:.4f}\")\n",
    "    # print(df.head())\n",
    "    # return y_pred, mae, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee8e5f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T20:07:40.143438Z",
     "iopub.status.busy": "2025-09-19T20:07:40.143093Z",
     "iopub.status.idle": "2025-09-19T20:07:40.173291Z",
     "shell.execute_reply": "2025-09-19T20:07:40.172382Z"
    },
    "papermill": {
     "duration": 0.036697,
     "end_time": "2025-09-19T20:07:40.174796",
     "exception": false,
     "start_time": "2025-09-19T20:07:40.138099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model (architecture must be the same as before)\n",
    "best_model = MLPWithAttention(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "# Load saved weights\n",
    "best_model.load_state_dict(torch.load(\"mlp_with_attention.pth\", map_location=device))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a507f535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T20:07:40.185091Z",
     "iopub.status.busy": "2025-09-19T20:07:40.184672Z",
     "iopub.status.idle": "2025-09-19T20:07:40.399732Z",
     "shell.execute_reply": "2025-09-19T20:07:40.398598Z"
    },
    "papermill": {
     "duration": 0.221839,
     "end_time": "2025-09-19T20:07:40.401168",
     "exception": false,
     "start_time": "2025-09-19T20:07:40.179329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.497358771639086\n",
      "Training -> MAE: 3.4974, MAPE: 0.0221, R2: 0.9515\n",
      "6.81597958633153\n",
      "Validation -> MAE: 6.8160, MAPE: 0.0344, R2: 0.8848\n",
      "5.784016855597359\n",
      "Test -> MAE: 5.7840, MAPE: 0.0368, R2: 0.9180\n"
     ]
    }
   ],
   "source": [
    "score_generator(X_train_tensor,y_train,best_model,\"Training\",is_torch=True,X_scaler=scaler_X,y_scaler=scaler_y)\n",
    "score_generator(X_val_tensor,y_val,best_model,\"Validation\",is_torch=True,X_scaler=scaler_X,y_scaler=scaler_y)\n",
    "score_generator(X_test_tensor,y_test,best_model,\"Test\",is_torch=True,X_scaler=scaler_X,y_scaler=scaler_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8694f8e3",
   "metadata": {
    "papermill": {
     "duration": 0.00493,
     "end_time": "2025-09-19T20:07:40.410665",
     "exception": false,
     "start_time": "2025-09-19T20:07:40.405735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8070685,
     "sourceId": 12766689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tansorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1363.323302,
   "end_time": "2025-09-19T20:07:43.274678",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-19T19:44:59.951376",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
